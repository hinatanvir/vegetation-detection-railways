{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1l8qd5egyO3z",
    "outputId": "c41a7c6a-0c41-4378-d95f-f4ed9c3941e4"
   },
   "outputs": [],
   "source": [
    "!pip install segmentation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGaY-5G4Ia6G"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import normalize\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TRJN49YuVYb",
    "outputId": "7cce0c47-b6d5-45f1-8863-980cba6097b0"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5RJZy_bzx2uw",
    "outputId": "fea18d7d-e2c7-46ee-8689-6dc23e0701f0"
   },
   "outputs": [],
   "source": [
    "temp_mask = cv2.imread('C:/Users/himan/Downloads/VegetationMonitoring/rs19_val/uint8/rs19_val/rs00000.png') #3 channels but all same. \n",
    "labels, count = np.unique(temp_mask[:,:,0], return_counts=True) #Check for each channel. All chanels are identical\n",
    "print(\"Labels are: \", labels, \" and the counts are: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_Jg6Gb-x3bs"
   },
   "outputs": [],
   "source": [
    "#width and length of the images\n",
    "w = temp_mask.shape[0]\n",
    "l = temp_mask.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "2jaT11Dux3ea",
    "outputId": "6414f0c7-ba56-485d-d487-83e95916fd8b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "#before processing\n",
    "plt.imshow(temp_mask[:,:,0], cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "Ra3BeJvDx3ht",
    "outputId": "a9d26f78-86d7-431c-b811-35082c7ac16f"
   },
   "outputs": [],
   "source": [
    "#temp_mask = cv2.imread('rC:/Users/himan/Downloads/VegetationMonitoring/rs19_val/uint8/rs19_val/rs00001.png')\n",
    "plt.figure(figsize = (10,5))\n",
    "label_of_interest = 8 #veg\n",
    "#after processing\n",
    "#temp_mask[:int(w/2),:,0]=0\n",
    "temp_mask[:,:,0][temp_mask[:,:,0]!=label_of_interest]=0\n",
    "plt.imshow(temp_mask[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nlg92VFIb49A",
    "outputId": "8a6422f4-867e-45d8-ba31-61b7d2ecb4ac"
   },
   "outputs": [],
   "source": [
    "len(np.unique(temp_mask[:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeIv085mNXjC"
   },
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12JIfFgHPB4x"
   },
   "outputs": [],
   "source": [
    "#test_image_data=scaler.fit_transform(image_dataset_uint8.reshape(-1, image_dataset_uint8.shape[-1])).reshape(image_dataset_uint8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "TcOeZ67pr-NJ",
    "outputId": "abe88424-07bf-40e4-980e-c9744df33b1a"
   },
   "outputs": [],
   "source": [
    "#these codes are used to loop through the directory\n",
    "path_images = 'C:/Users/himan/Downloads/VegetationMonitoring/rs19_val/jpgs/rs19_val/'\n",
    "path_mask = 'C:/Users/himan/Downloads/VegetationMonitoring/rs19_val/uint8/rs19_val/'\n",
    "breaker = 0\n",
    "\n",
    "image_dataset = []\n",
    "mask_dataset = []\n",
    "\n",
    "label_of_interest = 8 #veg\n",
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "for filename in os.listdir(path_images):\n",
    "    breaker +=1\n",
    "    if filename.endswith('.jpg'):  \n",
    "        image = cv2.imread(path_images+filename) \n",
    "        mask = cv2.imread(path_mask+filename[:7]+'.png')\n",
    "        for layer in range(len(mask[0,0,:])):\n",
    "            #mask[:int(w/2),:,layer]=0\n",
    "            mask[:,:,layer][mask[:,:,layer]!=label_of_interest]=0\n",
    "            mask[:,:,layer][mask[:,:,layer]==label_of_interest]=1\n",
    "                #this is because one hot encoder can only deal with 0 and 1 if there is 2 labels\n",
    "        # image = np.array(image)\n",
    "        # mask = np.array(mask)\n",
    "        image_dataset.append(image)\n",
    "        mask_dataset.append(mask)\n",
    "    if breaker==1800:\n",
    "         break\n",
    "\n",
    "image_dataset = np.array(image_dataset)\n",
    "mask_dataset = np.array(mask_dataset)\n",
    "mask_dataset = mask_dataset[:,:,:,0]\n",
    "print('image: ', image_dataset.shape)\n",
    "print('mask: ', mask_dataset.shape)\n",
    "plt.imshow(mask[:,:,0])\n",
    "\n",
    "mask_dataset = np.expand_dims(mask_dataset, axis=3)\n",
    "print('image: ', image_dataset.shape)\n",
    "print('mask: ', mask_dataset.shape)\n",
    "\n",
    "print('unique labels in mask', np.unique(mask_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEfm9pPEMmvL"
   },
   "outputs": [],
   "source": [
    "#Normalize images\n",
    "image_dataset = image_dataset /255.  #Can also normalize or scale using MinMax scaler\n",
    "#Do not normalize masks, just rescale to 0 to 1.\n",
    "mask_dataset = mask_dataset /255.  #PIxel values will be 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "az5lOo48Msva"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "tycUwzOoMtXZ",
    "outputId": "697cf6ec-028c-4ff3-f364-31d536a57f3a"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "image_number = random.randint(0, len(X_train)-1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_train[image_number,:,:,0], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(y_train[image_number,:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do_6g3dqMGEw"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.metrics import MeanIoU\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')\n",
    "sm.framework()\n",
    "from segmentation_models import Unet\n",
    "n_classes=1\n",
    "activation='sigmoid'\n",
    "\n",
    "#LR = 0.0001\n",
    "#optim = keras.optimizers.Adam(LR)\n",
    "optim='adam'\n",
    "\n",
    "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
    "#dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.25, 0.25])) \n",
    "#focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "#total_loss = dice_loss + (1 * focal_loss)\n",
    "total_loss = 'binary_crossentropy'\n",
    "\n",
    "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
    "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5),'accuracy']\n",
    "\n",
    "\n",
    "########################################################################\n",
    "###Model 1\n",
    "BACKBONE1 = 'resnet50'\n",
    "preprocess_input1 = sm.get_preprocessing(BACKBONE1)\n",
    "\n",
    "# preprocess input\n",
    "X_train1 = preprocess_input1(X_train)\n",
    "X_test1 = preprocess_input1(X_test)\n",
    "\n",
    "# define model\n",
    "model1 = sm.Unet(BACKBONE1, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
    "\n",
    "# compile keras model with defined optimozer, loss and metrics\n",
    "model1.compile(optim, total_loss, metrics=metrics)\n",
    "\n",
    "#model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
    "\n",
    "print(model1.summary())\n",
    "\n",
    "\n",
    "history1=model1.fit(X_train1, \n",
    "          y_train,\n",
    "          batch_size=16, \n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test1, y_test))\n",
    "\n",
    "\n",
    "############################################################\n",
    "###Model 2\n",
    "\n",
    "BACKBONE2 = 'inceptionv3'\n",
    "preprocess_input2 = sm.get_preprocessing(BACKBONE2)\n",
    "\n",
    "# preprocess input\n",
    "X_train2 = preprocess_input2(X_train)\n",
    "X_test2 = preprocess_input2(X_test)\n",
    "\n",
    "# define model\n",
    "model2 = sm.Unet(BACKBONE2, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
    "\n",
    "\n",
    "# compile keras model with defined optimozer, loss and metrics\n",
    "model2.compile(optim, total_loss, metrics)\n",
    "#model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
    "\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "\n",
    "history2=model2.fit(X_train2, \n",
    "          y_train,\n",
    "          batch_size=8, \n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test2, y_test))\n",
    "\n",
    "\n",
    "# #####################################################\n",
    "# ###Model 3\n",
    "\n",
    "# BACKBONE3 = 'vgg16'\n",
    "# preprocess_input3 = sm.get_preprocessing(BACKBONE3)\n",
    "\n",
    "# # preprocess input\n",
    "# X_train3 = preprocess_input3(X_train)\n",
    "# X_test3 = preprocess_input3(X_test)\n",
    "\n",
    "\n",
    "# # define model\n",
    "# model3 = sm.Unet(BACKBONE3, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
    "\n",
    "# # compile keras model with defined optimozer, loss and metrics\n",
    "# model3.compile(optim, total_loss, metrics)\n",
    "# #model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
    "\n",
    "\n",
    "# print(model3.summary())\n",
    "\n",
    "# history3=model3.fit(X_train3, \n",
    "#           y_train,\n",
    "#           batch_size=8, \n",
    "#           epochs=50,\n",
    "#           verbose=1,\n",
    "#           validation_data=(X_test3, y_test))\n",
    "##########################################################\n",
    "\n",
    "###\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history1.history['iou_score']\n",
    "val_acc = history1.history['val_iou_score']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation IOU')\n",
    "plt.title('Training and validation IOU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IOU')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#####################################################\n",
    "\n",
    "#IOU\n",
    "y_pred1=model2.predict(X_test2)\n",
    "y_pred1_argmax=np.argmax(y_pred1, axis=3)\n",
    "\n",
    "\n",
    "#Using built in keras function\n",
    "#from keras.metrics import MeanIoU\n",
    "n_class = 2\n",
    "IOU_keras = MeanIoU(num_classes=n_class)  \n",
    "IOU_keras.update_state(y_test[:,:,:,0], y_pred1_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "#To calculate I0U for each class...\n",
    "values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
    "print(values)\n",
    "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
    "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
    "\n",
    "\n",
    "print(\"IoU for class1 is: \", class1_IoU)\n",
    "print(\"IoU for class2 is: \", class2_IoU)\n",
    "\n",
    "#Vaerify the prediction on first image\n",
    "plt.imshow(train_images[0, :,:,0], cmap='gray')\n",
    "plt.imshow(train_masks[0], cmap='gray')\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test some random images\n",
    "import random\n",
    "test_img_number = random.randint(0, len(X_test2))\n",
    "test_img = X_test2[test_img_number]\n",
    "ground_truth=y_test[test_img_number]\n",
    "test_img_input=np.expand_dims(test_img, 0)\n",
    "test_img_input1 = preprocess_input2(test_img_input)\n",
    "\n",
    "test_pred1 = model2.predict(test_img_input1)\n",
    "test_prediction1 = np.argmax(test_pred1, axis=3)[0,:,:]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(test_prediction1, cmap='gray')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial118_binary_semantic_segmentation_using_unet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "518071c88b4194aee059da36c8bb8c7f03edabff5e31c99a4f47aa39aa99e517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
